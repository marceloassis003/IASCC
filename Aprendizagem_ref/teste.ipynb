{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import random\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_accao(self, s):\n",
    "        random.shuffle(self.accoes)\n",
    "        return max(self.accoes, lambda a: MemoriaAprend.Q(s, a))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'function' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m6\u001b[39m , \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m      4\u001b[0m atp \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mshuffle(data)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mMemoriaAprend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(atp)\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'function' and 'list'"
     ]
    }
   ],
   "source": [
    "data = [3, 4, 6 , 7, 2, 5]\n",
    "\n",
    "class Accao(ABC):\n",
    "    def __init__(self, accoes: list):\n",
    "        self.accoes = accoes\n",
    "        return self.accoes\n",
    "\n",
    "class Estado(ABC):\n",
    "    def __init__(self):\n",
    "        self.Estado = Estado \n",
    "        return self.Estado\n",
    "\n",
    "\n",
    "class MemoriaAprend(ABC):\n",
    "    def __init__(self, s: Estado, a: Accao, q:float):\n",
    "        self.s = s\n",
    "        self.a = a\n",
    "        self.q = q\n",
    "\n",
    "    @abstractmethod\n",
    "    def actualizar(self, s, a, q):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def Q(self, s, a):\n",
    "        pass \n",
    "\n",
    "\n",
    "\n",
    "atp = random.shuffle(data)\n",
    "max(data, lambda a: MemoriaAprend.Q(s, a))\n",
    "\n",
    "print(atp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n",
      "ganhou !!!!!!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marce\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"------------------------------------------------------------------------------------------------------------------------------------------------------------------ \n",
    "#\n",
    "#                                                                   V1 - GAME - CHEGAR NO CIRCULO \n",
    "# Objectivo: Desviar dos obtaculos e chegar no circulo \n",
    "#_____________________________________________________________________________________________________________________________________________________________________________\"\"\"\n",
    "\n",
    "\n",
    "import pygame\n",
    "from pygame.locals import *\n",
    "from sys import exit\n",
    "\n",
    "\n",
    "pygame.init()\n",
    "\n",
    "height = 640  \n",
    "width = 550\n",
    "\n",
    "#x = 30\n",
    "#y = 30\n",
    "\n",
    "screen = pygame.display.set_mode((height, width))\n",
    "pygame.display.set_caption(\"Aprendizagem por reforço\")\n",
    "\n",
    "# settigns agente \n",
    "\n",
    "x, y = 100, 100\n",
    "vel = 0.5\n",
    "agente_width = 20\n",
    "agente_height = 30 \n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "   \n",
    "\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == QUIT:\n",
    "            pygame.quit()\n",
    "            exit()\n",
    "    \n",
    "    # movimento agente\n",
    "    keys = pygame.key.get_pressed()\n",
    "    move_x, move_y = 0,0 \n",
    "\n",
    "    if pygame.key.get_pressed()[K_a]:\n",
    "        move_x = - vel\n",
    "        #x = x - 20 \n",
    "    if pygame.key.get_pressed()[K_d]:\n",
    "        move_x = + vel\n",
    "        #x = x + 20 \n",
    "    if pygame.key.get_pressed()[K_w]:\n",
    "        move_y = - vel\n",
    "        #y = y - 20 \n",
    "    if pygame.key.get_pressed()[K_s]:\n",
    "        move_y = + vel\n",
    "        #y = y + 20 \n",
    "\n",
    "    # atualizando \n",
    "    new_x = x + move_x\n",
    "    new_y = y + move_y\n",
    "\n",
    "    screen.fill((0, 0, 0))\n",
    "    \n",
    "    # agente \n",
    "    agente = pygame.draw.circle(screen, (0,255,255), (new_x, new_y), agente_width)\n",
    "   \n",
    "    \n",
    "    # parede C invertido \n",
    "    ob1 = pygame.draw.rect(screen, (255,255,0), (width//2 + 40 , height//2 - 200, 50, 300))\n",
    "    ob2 = pygame.draw.rect(screen, (255,255,0), (width//2 - 40, height//2 - 200, 80, 30))\n",
    "    ob3 = pygame.draw.rect(screen, (255,255,0), (width//2 - 40, height//2 + 70, 80, 30))\n",
    "\n",
    "    # objectivo final \n",
    "    objectivo = pygame.draw.circle(screen, (0,255,0), (550, 260), 30)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    if agente.colliderect(ob1) or agente.colliderect(ob2) or agente.colliderect(ob3) :\n",
    "        new_x = x \n",
    "        new_y = y\n",
    "        #x = x\n",
    "        #y = y\n",
    "    \n",
    "    if agente.colliderect(objectivo):\n",
    "        print(\"ganhou !!!!!!\")\n",
    "        #pygame.quit()\n",
    "        \n",
    "    # atualiza posição \n",
    "    x,y = new_x, new_y\n",
    "\n",
    "\n",
    "\n",
    "    pygame.display.update()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "640/2\n",
    "#480/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marce\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"------------------------------------------------------------------------------------------------------------------------------------------------------------------ \n",
    "# IMPLENTAÇÃO DA AI - ALGORITMO POR REFORÇO \n",
    "#                                                                   V2 - GAME - CHEGAR NO CIRCULO\n",
    "# Objectivo: Desviar dos obtaculos e chegar no circulo \n",
    "#_____________________________________________________________________________________________________________________________________________________________________________\"\"\"\n",
    "\n",
    "\n",
    "import pygame\n",
    "from pygame.locals import *\n",
    "from sys import exit\n",
    "import random \n",
    "import numpy as np \n",
    "from pratica_pt2 import Accao, MemoriaEsparsa, EGreedy, QLearning, DynaQ\n",
    "\n",
    "# settings screen \n",
    "height = 640  \n",
    "width = 550\n",
    "\n",
    "\n",
    "# criando classes \n",
    "\n",
    "class Ambiente:\n",
    "    def __init__(self, height, width, vel,  agente_width, agente_height):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.vel = vel\n",
    "        self.agente_width = agente_width\n",
    "        self.agente_height = agente_height\n",
    "        self.screen = pygame.display.set_mode((height, width))\n",
    "        pygame.display.set_caption(\"Aprendizagem por reforço\")\n",
    "        self.recompensa = np.zeros((int(height//vel), int(width//vel)))\n",
    "        self.recompensas[26][54] = 10\n",
    "        for i in range(21, 35):\n",
    "            self.recompensas[i][32] = -1\n",
    "\n",
    "    def mapeando_ambiente(self, agente_pos):\n",
    "        # frame screen \n",
    "        self.screen.fill((0, 0, 0))\n",
    "        # agente circulo ciano \n",
    "        self.agente = pygame.draw.circle(self.screen, (0,255,255), (agente_pos), self.agente_width)\n",
    "        # obstaculo c invertido \n",
    "        self.ob1 = pygame.draw.rect(self.screen, (255,255,0), (self.width//2 + 40 , self.height//2 - 200, 50, 300))\n",
    "        self.ob2 = pygame.draw.rect(self.screen, (255,255,0), (self.width//2 - 40, self.height//2 - 200, 80, 30))\n",
    "        self.ob3 = pygame.draw.rect(self.screen, (255,255,0), (self.width//2 - 40, self.height//2 + 70, 80, 30))\n",
    "        # circulo objectivo \n",
    "        objectivo = pygame.draw.circle(self.screen, (0,255,0), (550, 260), 30)\n",
    "\n",
    "    def obter_estado(self, pos):\n",
    "        return (int(pos[0] // vel), int(pos[1] // vel))\n",
    "    \n",
    "    def passo(self, acao, pos):\n",
    "        move_x, move_y = 0, 0 \n",
    "        if acao == 'esquerda':\n",
    "            move_x = - self.vel\n",
    "        elif acao == 'direita':\n",
    "            move_x = self.vel\n",
    "        elif acao == 'cima':\n",
    "            move_y = - self.vel\n",
    "        elif acao == 'baixo':\n",
    "            move_y = self.vel\n",
    "\n",
    "        new_x = min(max(pos[0] + move_x, 0), self.width - self.agente_width)\n",
    "        new_y = min(max(pos[1] + move_y, 0), self.height - self.agente_height)\n",
    "        new_pos = (new_x, new_y)\n",
    "        novo_estado = self.obter_estado(new_pos)\n",
    "        recompensa = self.recompensas[novo_estado[0]][novo_estado[1]]\n",
    "\n",
    "        # analise de espaço do agente \n",
    "        #if pygame.self.agente.colliderect(self.ob1) or agente.colliderect(ob2) or agente.colliderect(ob3)\n",
    "\n",
    "        return new_pos, recompensa\n",
    "    \n",
    "class Agente:\n",
    "    def __init__(self, ambiente, x, y, vel):\n",
    "        self.ambiente = ambiente\n",
    "        self.x = x\n",
    "        self.y = y \n",
    "        self.vel = vel\n",
    "        self.mem_aprend = MemoriaEsparsa()              # criação setting moving \n",
    "        self.accoes = [Accao(['esquerda']), Accao(['direita']), Accao(['cima']), Accao(['baixo'])] \n",
    "        self.sel_accao = EGreedy(self.mem_aprend, self.accoes, epsilon)\n",
    "        self.agente = QLearning(self.mem_aprend, self.sel_accao, alfa, gama )\n",
    "\n",
    "    def treinar(self, num_epoca):\n",
    "        for epsilon in range(num_epoca):\n",
    "            estado = self.ambiente.obter_estado((self.x, self.y))\n",
    "            accao = self.sel_accao.seleccionar_accao(estado)\n",
    "            for _ in range(200):\n",
    "                novo_estado, recompensa = self.ambiente.passo(accao.accoes[0], (self.x, self.y))\n",
    "                nova_accao = self.sel_accao.seleccionar_accao(novo_estado)\n",
    "                self.agente.aprender(estado, accao, recompensa, novo_estado)\n",
    "                estado = novo_estado\n",
    "                accao = nova_accao\n",
    "                if recompensa == 10:\n",
    "                    break\n",
    "\n",
    "    def mover(self):\n",
    "        estado = self.ambiente.obter_estado((self.x, self.y))\n",
    "        accao = self.sel_accao.seleccionar_accao(estado)\n",
    "        novo_estado, recompensa = self.ambiente.passo(accao.accoes[0], (self.x, self.y))      \n",
    "        self.x, self.y  = novo_estado[0] * self.vel, novo_estado\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pygame.init()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# settigns agente \n",
    "\n",
    "x, y = 100, 100\n",
    "vel = 10\n",
    "agente_width = 20\n",
    "agente_height = 30 \n",
    "\n",
    "# parametros de learnign \n",
    "\n",
    "dim_max = 1000\n",
    "num_sim = 10\n",
    "alfa = 0.1\n",
    "gama = 0.9\n",
    "epsilon = 0.9\n",
    "num_epoca = 10000\n",
    "\n",
    "\n",
    "\n",
    "# Chamada metodo de reforço \n",
    "agente = QLearning(mem_aprend, sel_accao, alfa, gama)\n",
    "\n",
    "# iniciando recompensas para o agente \n",
    "estado_inicial = (x, y)\n",
    "\n",
    "\n",
    "# definir recompensas para o objectivo \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "   \n",
    "# treinar o algoritmo \n",
    "for episodio in range(num_epoca):\n",
    "    estado = obter_estado((x, y))\n",
    "    accao = sel_accao.seleccionar_accao(estado)\n",
    "    for _ in range(200):\n",
    "        novo_estado, recompensa = passo(accao.accoes[0], (x, y))\n",
    "        nova_accao = sel_accao.seleccionar_accao(novo_estado)\n",
    "        agente.aprender(estado, accao, recompensa, novo_estado)\n",
    "        estado = novo_estado\n",
    "        accao = nova_accao\n",
    "        if recompensa == 10:\n",
    "            break\n",
    "\n",
    "\n",
    "# execução do game \n",
    "\n",
    "while True:\n",
    "   \n",
    "\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == QUIT:\n",
    "            pygame.quit()\n",
    "            exit()\n",
    "\n",
    "    # settings algoritmo \n",
    "    estado = obter_estado((x, y))\n",
    "    accao = sel_accao.seleccionar_accao(estado)#greedy=True\n",
    "    novo_estado, _ = passo(accao.accoes[0], (x, y))\n",
    "    x, y = novo_estado[0] * vel, novo_estado[1] * vel\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \"\"\" \n",
    "    # movimento agente\n",
    "    keys = pygame.key.get_pressed()\n",
    "    move_x, move_y = 0,0 \n",
    "\n",
    "    if pygame.key.get_pressed()[K_a]:\n",
    "        move_x = - vel\n",
    "        #x = x - 20 \n",
    "    if pygame.key.get_pressed()[K_d]:\n",
    "        move_x = + vel\n",
    "        #x = x + 20 \n",
    "    if pygame.key.get_pressed()[K_w]:\n",
    "        move_y = - vel\n",
    "        #y = y - 20 \n",
    "    if pygame.key.get_pressed()[K_s]:\n",
    "        move_y = + vel\n",
    "        #y = y + 20 \n",
    "\n",
    "    # atualizando \n",
    "    new_x = x + move_x\n",
    "    new_y = y + move_y\n",
    "    \n",
    "    if agente.colliderect(ob1) or agente.colliderect(ob2) or agente.colliderect(ob3) :\n",
    "        new_x = x \n",
    "        new_y = y\n",
    "        #x = x\n",
    "        #y = y\n",
    "    \n",
    "    if agente.colliderect(objectivo):\n",
    "        print(\"ganhou !!!!!!\")\n",
    "        #pygame.quit()\n",
    "        \n",
    "    # atualiza posição \n",
    "    x,y = new_x, new_y\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "    pygame.display.update()\n",
    "    pygame.time.delay(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 55 is out of bounds for axis 0 with size 55",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 99\u001b[0m\n\u001b[0;32m     97\u001b[0m estado \u001b[38;5;241m=\u001b[39m obter_estado((x, y))\n\u001b[0;32m     98\u001b[0m accao \u001b[38;5;241m=\u001b[39m sel_accao\u001b[38;5;241m.\u001b[39mseleccionar_accao(estado)  \u001b[38;5;66;03m# Usar a política aprendida\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m novo_estado, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpasso\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccao\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccoes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m x, y \u001b[38;5;241m=\u001b[39m novo_estado[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m vel, novo_estado[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m vel\n\u001b[0;32m    102\u001b[0m screen\u001b[38;5;241m.\u001b[39mfill((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "Cell \u001b[1;32mIn[3], line 74\u001b[0m, in \u001b[0;36mpasso\u001b[1;34m(acao, pos)\u001b[0m\n\u001b[0;32m     72\u001b[0m new_pos \u001b[38;5;241m=\u001b[39m (new_x, new_y)\n\u001b[0;32m     73\u001b[0m novo_estado \u001b[38;5;241m=\u001b[39m obter_estado(new_pos)\n\u001b[1;32m---> 74\u001b[0m recompensa \u001b[38;5;241m=\u001b[39m \u001b[43mrecompensas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnovo_estado\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnovo_estado\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m novo_estado, recompensa\n",
      "\u001b[1;31mIndexError\u001b[0m: index 55 is out of bounds for axis 0 with size 55"
     ]
    }
   ],
   "source": [
    "\"\"\"------------------------------------------------------------------------------------------------------------------------------------------------------------------ \n",
    "# IMPLENTAÇÃO DA AI - ALGORITMO POR REFORÇO \n",
    "#                                                                   V2 - GAME - CHEGAR NO CIRCULO\n",
    "# Objectivo: Desviar dos obtaculos e chegar no circulo \n",
    "# \n",
    "# OBS: versão funcionando porem sem estructura de agente e ambiente em classes , e com erros ao agenter explorar fora da margem \n",
    "#_____________________________________________________________________________________________________________________________________________________________________________\"\"\"\n",
    "\n",
    "\n",
    "import pygame\n",
    "from pygame.locals import *\n",
    "from sys import exit\n",
    "import random\n",
    "import numpy as np\n",
    "from pratica_pt2 import Accao, MemoriaEsparsa, EGreedy, QLearning, DynaQ\n",
    "\n",
    "pygame.init()\n",
    "\n",
    "height = 640  \n",
    "width = 550\n",
    "\n",
    "screen = pygame.display.set_mode((height, width))\n",
    "pygame.display.set_caption(\"Aprendizagem por reforço\")\n",
    "\n",
    "# Configurações do agente\n",
    "x, y = 100, 100\n",
    "vel = 10\n",
    "agente_width = 20\n",
    "agente_height = 20\n",
    "\n",
    "# Parâmetros de aprendizagem\n",
    "dim_max = 1000\n",
    "num_sim = 10\n",
    "alfa = 0.1\n",
    "gama = 0.9\n",
    "epsilon = 1\n",
    "num_episodios = 10000\n",
    "\n",
    "# Criando a memória de aprendizagem e seleção de ação (e-greedy)\n",
    "mem_aprend = MemoriaEsparsa() \n",
    "accoes = [Accao(['esquerda']), Accao(['direita']), Accao(['cima']), Accao(['baixo'])]\n",
    "sel_accao = EGreedy(mem_aprend, accoes, epsilon)\n",
    "\n",
    "# Alteração para QLearning\n",
    "agente = QLearning(mem_aprend, sel_accao, alfa, gama)\n",
    "\n",
    "# Inicializar estado e recompensas\n",
    "estado_inicial = (x, y)\n",
    "recompensas = np.zeros((int(height//vel), int(width//vel)))  # Convertendo para inteiros\n",
    "\n",
    "# Definir recompensas para o objetivo e penalidade para obstáculos\n",
    "recompensas[26][54] = 10  # Objetivo\n",
    "for i in range(21, 35):\n",
    "    recompensas[i][32] = -1  # Obstáculos C invertido\n",
    "\n",
    "def obter_estado(pos):\n",
    "    return (int(pos[0] // vel), int(pos[1] // vel))\n",
    "\n",
    "def passo(acao, pos):\n",
    "    move_x, move_y = 0, 0\n",
    "    if acao == 'esquerda':\n",
    "        move_x = -vel\n",
    "    elif acao == 'direita':\n",
    "        move_x = vel\n",
    "    elif acao == 'cima':\n",
    "        move_y = -vel\n",
    "    elif acao == 'baixo':\n",
    "        move_y = vel\n",
    "\n",
    "    new_x = min(max(pos[0] + move_x, 0), width - agente_width)\n",
    "    new_y = min(max(pos[1] + move_y, 0), height - agente_height)\n",
    "    new_pos = (new_x, new_y)\n",
    "    novo_estado = obter_estado(new_pos)\n",
    "    recompensa = recompensas[novo_estado[0]][novo_estado[1]]\n",
    "    return novo_estado, recompensa\n",
    "\n",
    "# Treinamento\n",
    "for episodio in range(num_episodios):\n",
    "    estado = obter_estado((x, y))\n",
    "    accao = sel_accao.seleccionar_accao(estado)\n",
    "    for _ in range(200):\n",
    "        novo_estado, recompensa = passo(accao.accoes[0], (x, y))\n",
    "        nova_accao = sel_accao.seleccionar_accao(novo_estado)\n",
    "        agente.aprender(estado, accao, recompensa, novo_estado)\n",
    "        estado = novo_estado\n",
    "        accao = nova_accao\n",
    "        if recompensa == 10:  # Objetivo alcançado\n",
    "            break\n",
    "\n",
    "# Executar o jogo com a política aprendida\n",
    "while True:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == QUIT:\n",
    "            pygame.quit()\n",
    "            exit()\n",
    "\n",
    "    estado = obter_estado((x, y))\n",
    "    accao = sel_accao.seleccionar_accao(estado)  # Usar a política aprendida\n",
    "    novo_estado, _ = passo(accao.accoes[0], (x, y))\n",
    "    x, y = novo_estado[0] * vel, novo_estado[1] * vel\n",
    "\n",
    "    screen.fill((0, 0, 0))\n",
    "    \n",
    "    # agente \n",
    "    pygame.draw.circle(screen, (0,255,255), (x, y), agente_width)\n",
    "    \n",
    "    # parede C invertido \n",
    "    pygame.draw.rect(screen, (255,255,0), (width//2 + 40 , height//2 - 200, 50, 300))\n",
    "    pygame.draw.rect(screen, (255,255,0), (width//2 - 40, height//2 - 200, 80, 30))\n",
    "    pygame.draw.rect(screen, (255,255,0), (width//2 - 40, height//2 + 70, 80, 30))\n",
    "\n",
    "    # objectivo final \n",
    "    pygame.draw.circle(screen, (0,255,0), (550, 260), 30)\n",
    "\n",
    "    pygame.display.update()\n",
    "    pygame.time.delay(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 227\u001b[0m\n\u001b[0;32m    224\u001b[0m agente \u001b[38;5;241m=\u001b[39m Agente(ambiente)\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# Treinamento do agente\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m \u001b[43magente\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtreinar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNUM_EPISODIOS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# Loop principal\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[1], line 204\u001b[0m, in \u001b[0;36mAgente.treinar\u001b[1;34m(self, num_episodios)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;66;03m#print(self.mem_aprend.memoria)\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m#print(estado)\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m#print(recompensa)\u001b[39;00m\n\u001b[0;32m    203\u001b[0m estado \u001b[38;5;241m=\u001b[39m novo_estado\n\u001b[1;32m--> 204\u001b[0m accao \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msel_accao\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseleccionar_accao\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnovo_estado\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marce\\OneDrive\\Desktop\\Masters_degree\\IASCC\\Aprendizagem_ref\\pratica_pt2.py:143\u001b[0m, in \u001b[0;36mEGreedy.seleccionar_accao\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mseleccionar_accao\u001b[39m(\u001b[38;5;28mself\u001b[39m, s):\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisolon:\n\u001b[1;32m--> 143\u001b[0m         accao \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maproveitar\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m         accao \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplorar()\n",
      "File \u001b[1;32mc:\\Users\\marce\\OneDrive\\Desktop\\Masters_degree\\IASCC\\Aprendizagem_ref\\pratica_pt2.py:136\u001b[0m, in \u001b[0;36mEGreedy.aproveitar\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maproveitar\u001b[39m(\u001b[38;5;28mself\u001b[39m, s):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_accao\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marce\\OneDrive\\Desktop\\Masters_degree\\IASCC\\Aprendizagem_ref\\pratica_pt2.py:133\u001b[0m, in \u001b[0;36mEGreedy.max_accao\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax_accao\u001b[39m(\u001b[38;5;28mself\u001b[39m, s: Estado) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Accao:\n\u001b[0;32m    132\u001b[0m     random\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccoes)\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccoes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmem_aprend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marce\\OneDrive\\Desktop\\Masters_degree\\IASCC\\Aprendizagem_ref\\pratica_pt2.py:133\u001b[0m, in \u001b[0;36mEGreedy.max_accao.<locals>.<lambda>\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax_accao\u001b[39m(\u001b[38;5;28mself\u001b[39m, s: Estado) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Accao:\n\u001b[0;32m    132\u001b[0m     random\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccoes)\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccoes, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m a: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem_aprend\u001b[38;5;241m.\u001b[39mQ(s, a))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "from pygame.locals import *\n",
    "from sys import exit\n",
    "import numpy as np\n",
    "from pratica_pt2 import Accao, MemoriaEsparsa, EGreedy, QLearning\n",
    "\n",
    "# Configurações do jogo\n",
    "ALTURA = 640\n",
    "LARGURA = 550\n",
    "VEL = 10\n",
    "RAIO_AGENTE = 20\n",
    "RAIO_OBJECTIVO = 30\n",
    "NUM_EPISODIOS = 1000\n",
    "ALFA = 0.1\n",
    "GAMA = 0.9\n",
    "EPSILON = 0.1\n",
    "\n",
    "# Cores\n",
    "PRETO = (0, 0, 0)\n",
    "AMARELO = (255, 255, 0)\n",
    "VERDE = (0, 255, 0)\n",
    "AZUL = (0, 255, 255)\n",
    "\n",
    "\n",
    "class Ambiente:\n",
    "    def __init__(self, altura, largura, vel):\n",
    "        self.altura = altura\n",
    "        self.largura = largura\n",
    "        self.vel = vel\n",
    "        self.recompensas = np.zeros((altura // vel, largura // vel))  # Matriz de recompensas\n",
    "        self.obstaculos = []  # Lista de retângulos de obstáculos\n",
    "        \n",
    "\n",
    "        # Definir o objetivo antes de criar o Rect\n",
    "        self.objetivo = (27, 54)  # Posição do objetivo (linha, coluna)\n",
    "        self.recompensas[self.objetivo[0]][self.objetivo[1]] = 10\n",
    "\n",
    "        # Criar um Rect para o objetivo\n",
    "        self.objectivo = pygame.Rect(\n",
    "            self.objetivo[1] * vel,  # Coordenada x\n",
    "            self.objetivo[0] * vel,  # Coordenada y\n",
    "            vel,  # Largura\n",
    "            vel   # Altura\n",
    "        )\n",
    "\n",
    "        # Inicializar o ambiente\n",
    "        self.configurar_ambiente()\n",
    "\n",
    "    def configurar_ambiente(self):\n",
    "        # Definir o objetivo\n",
    "        self.objetivo = (27, 54)  # Posição do objetivo (matriz)\n",
    "        self.recompensas[self.objetivo[0]][self.objetivo[1]] = 10\n",
    "\n",
    "        # Criar os obstáculos (C invertido)\n",
    "        for i in range(21, 45):\n",
    "            self.recompensas[i][32] = -1  # Penalização para bater na parede\n",
    "            self.obstaculos.append(pygame.Rect(32 * VEL, i * VEL, VEL, VEL))\n",
    "\n",
    "        self.obstaculos.append(pygame.Rect(30 * VEL , 21 * VEL, 3 * VEL, VEL))  # Parte superior\n",
    "        self.obstaculos.append(pygame.Rect(30 * VEL , 44 * VEL, 3 * VEL, VEL))  # Parte inferior\n",
    "\n",
    "        # Adicionar margens como obstáculos\n",
    "        margem_espessura = 5  # Ajuste conforme necessário\n",
    "\n",
    "        # Margem superior\n",
    "        self.obstaculos.append(pygame.Rect(0, 0, self.largura + 100, margem_espessura))\n",
    "        # Margem inferior\n",
    "        self.obstaculos.append(pygame.Rect(0, self.altura - 95, self.largura + 100, margem_espessura))\n",
    "        # Margem esquerda\n",
    "        self.obstaculos.append(pygame.Rect(0, 0, margem_espessura, self.altura))\n",
    "        # Margem direita\n",
    "        self.obstaculos.append(pygame.Rect(self.largura + 85, 0, margem_espessura, self.altura))\n",
    "\n",
    "        # Atualizar penalizações para as bordas na matriz de recompensas\n",
    "        for i in range(self.altura // self.vel):  # Altura em passos\n",
    "            for j in range(self.largura // self.vel):  # Largura em passos\n",
    "                # Penalizar células na margem superior ou inferior\n",
    "                if i < margem_espessura // self.vel or i >= (self.altura - margem_espessura) // self.vel:\n",
    "                    self.recompensas[i][j] = -10\n",
    "                # Penalizar células na margem esquerda ou direita\n",
    "                if j < margem_espessura // self.vel or j >= (self.largura - margem_espessura) // self.vel:\n",
    "                    self.recompensas[i][j] = -10\n",
    "\n",
    "    def desenhar(self, tela):\n",
    "        \n",
    "        # Desenhar obstáculos e margem\n",
    "        for obstaculo in self.obstaculos:\n",
    "            pygame.draw.rect(tela, AMARELO, obstaculo)\n",
    "\n",
    "\n",
    "        # Desenhar o objetivo\n",
    "        objetivo_centro = (self.objetivo[1] * VEL + VEL // 2, self.objetivo[0] * VEL + VEL // 2)\n",
    "        pygame.draw.circle(tela, VERDE, objetivo_centro, RAIO_OBJECTIVO)\n",
    "        #self.objectivo = pygame.Rect(self.objetivo[1] * VEL, self.objetivo[0] * VEL, VEL, VEL)\n",
    "\n",
    "    def verificar_colisao(self, x, y):\n",
    "        # Verificar colisão com os obstáculos\n",
    "        for obstaculo in self.obstaculos:\n",
    "            if obstaculo.collidepoint(x, y):\n",
    "                x = x\n",
    "                y = y \n",
    "                return x,y\n",
    "                #return True\n",
    "        return False\n",
    "    \n",
    "    def obter_estado(self, pos):\n",
    "        return (pos[1] // self.vel, pos[0] // self.vel)\n",
    "\n",
    "    def obter_recompensa(self, estado):\n",
    "        if estado[0] < 0 or estado[0] >= self.altura //self.vel or estado[1] < 0 or estado[1] >= self.largura // self.vel:\n",
    "            return - 10\n",
    "        return self.recompensas[estado[0]][estado[1]]\n",
    "    \n",
    "\n",
    "\n",
    "class Agente:\n",
    "    def __init__(self, ambiente):\n",
    "        self.ambiente = ambiente\n",
    "        self.x = 100\n",
    "        self.y = 100\n",
    "        self.mem_aprend = MemoriaEsparsa()\n",
    "        self.accoes = [\n",
    "            Accao(['esquerda']),\n",
    "            Accao(['direita']),\n",
    "            Accao(['cima']),\n",
    "            Accao(['baixo']),\n",
    "        ]\n",
    "\n",
    "        self.sel_accao = EGreedy(self.mem_aprend, self.accoes, EPSILON)\n",
    "        self.qlearning = QLearning(self.mem_aprend, self.sel_accao, ALFA, GAMA)\n",
    "\n",
    "    def mover(self, acao):\n",
    "        move_x, move_y = 0, 0\n",
    "        if acao == 'esquerda':\n",
    "            move_x = -VEL\n",
    "        elif acao == 'direita':\n",
    "            move_x = VEL\n",
    "        elif acao == 'cima':\n",
    "            move_y = -VEL\n",
    "        elif acao == 'baixo':\n",
    "            move_y = VEL\n",
    "\n",
    "        novo_x = self.x + move_x\n",
    "        novo_y = self.y + move_y\n",
    "\n",
    "        margem_espessura = 5\n",
    "        novo_x = max(margem_espessura + RAIO_AGENTE, min(novo_x, self.ambiente.largura - margem_espessura - RAIO_AGENTE))\n",
    "        novo_y = max(margem_espessura + RAIO_AGENTE, min(novo_y, self.ambiente.altura - margem_espessura - RAIO_AGENTE))\n",
    "\n",
    "        \n",
    "        # Verificar colisão com obstáculos\n",
    "        if self.ambiente.verificar_colisao(novo_x, novo_y):\n",
    "            return self.x, self.y  # Não se move ao colidir\n",
    "\n",
    "        return novo_x, novo_y\n",
    "\n",
    "    def treinar(self, num_episodios):\n",
    "        for episodio in range(num_episodios):\n",
    "            self.x, self.y = 100, 100  # Reiniciar posição inicial\n",
    "            estado = self.ambiente.obter_estado((self.x, self.y))\n",
    "            accao = self.sel_accao.seleccionar_accao(estado)\n",
    "            total_recompensa = 0 \n",
    "            passos = 0\n",
    "            \"\"\" \n",
    "            for _ in range(num_episodios):\n",
    "                novo_x, novo_y = self.mover(accao.accoes[0])\n",
    "                novo_estado = self.ambiente.obter_estado((novo_x, novo_y))\n",
    "                recompensa = self.ambiente.obter_recompensa(novo_estado)\n",
    "\n",
    "                if self.ambiente.objectivo.colliderect(pygame.Rect(novo_x, novo_y, RAIO_AGENTE, RAIO_AGENTE)):\n",
    "                    recompensa = 10  # Recompensa máxima ao atingir o objetivo\n",
    "                    self.qlearning.aprender(estado, accao, recompensa, novo_estado)\n",
    "                    break  # Encerrar o episódio\n",
    "            \"\"\" \n",
    "            while True:\n",
    "                novo_x, novo_y = self.mover(accao.accoes[0])\n",
    "                novo_estado = self.ambiente.obter_estado((novo_x, novo_y))\n",
    "                recompensa = self.ambiente.obter_recompensa(novo_estado)\n",
    "\n",
    "                # atualiza a cada passo \n",
    "                total_recompensa += recompensa\n",
    "                passos += 1 \n",
    "\n",
    "                # Verificar objetivo final \n",
    "                if self.ambiente.objectivo.colliderect(pygame.Rect(novo_x, novo_y, RAIO_AGENTE, RAIO_AGENTE)):\n",
    "                    recompensa = 10  # Recompensa máxima ao atingir o objetivo\n",
    "                    self.qlearning.aprender(estado, accao, recompensa, novo_estado)\n",
    "                    #print(f\"Episódio {episodio + 1}: Objetivo alcançado em {passos} passos com recompensa total {total_recompensa}.\")\n",
    "                    break  # Encerrar o episódio\n",
    "\n",
    "\n",
    "                                        \n",
    "\n",
    "\n",
    "\n",
    "                #nova_accao = self.sel_accao.seleccionar_accao(novo_estado)\n",
    "                self.qlearning.aprender(estado, accao, recompensa, novo_estado)\n",
    "\n",
    "                self.x, self.y = novo_x, novo_y\n",
    "                #print(self.mem_aprend.memoria)\n",
    "                #print(estado)\n",
    "                #print(recompensa)\n",
    "                estado = novo_estado\n",
    "                accao = self.sel_accao.seleccionar_accao(novo_estado)\n",
    "                # Monitorar progresso logs episódios\n",
    "                #if (episodio + 1) % 100 == 0:\n",
    "                    #print(f\"Episódio {episodio + 1}: Total de estados visitados = {len(self.mem_aprend.obter_memoria())}\")\n",
    " \n",
    "\n",
    "    def executar_politica(self):\n",
    "        estado = self.ambiente.obter_estado((self.x, self.y))\n",
    "        accao = self.sel_accao.seleccionar_accao(estado)  # Política aprendida\n",
    "        self.x, self.y = self.mover(accao.accoes[0])\n",
    "\n",
    "\n",
    "# Configuração do jogo\n",
    "pygame.init()\n",
    "screen = pygame.display.set_mode((ALTURA, LARGURA))\n",
    "pygame.display.set_caption(\"Aprendizagem por Reforço\")\n",
    "relogio = pygame.time.Clock()\n",
    "\n",
    "# Inicializar ambiente e agente\n",
    "ambiente = Ambiente(ALTURA, LARGURA, VEL)\n",
    "agente = Agente(ambiente)\n",
    "\n",
    "# Treinamento do agente\n",
    "agente.treinar(NUM_EPISODIOS)\n",
    "\n",
    "# Loop principal\n",
    "while True:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == QUIT:\n",
    "            pygame.quit()\n",
    "            exit()\n",
    "\n",
    "    # Agente segue a política aprendida\n",
    "    agente.executar_politica()\n",
    "\n",
    "    # Desenhar na tela\n",
    "    screen.fill(PRETO)\n",
    "    ambiente.desenhar(screen)\n",
    "    pygame.draw.circle(screen, AZUL, (agente.x, agente.y), RAIO_AGENTE)\n",
    "    pygame.display.update()\n",
    "    relogio.tick(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "from pygame.locals import *\n",
    "from sys import exit\n",
    "import numpy as np\n",
    "from pratica_pt2 import Accao, MemoriaEsparsa, EGreedy, QLearning, DynaQ\n",
    "\n",
    "# Configurações do jogo\n",
    "ALTURA = 640\n",
    "LARGURA = 550\n",
    "VEL = 10\n",
    "RAIO_AGENTE = 20\n",
    "RAIO_OBJECTIVO = 30\n",
    "NUM_EPISODIOS = 1000\n",
    "NUM_SIM = 10\n",
    "ALFA = 0.1\n",
    "GAMA = 0.9\n",
    "EPSILON = 0.1\n",
    "\n",
    "# Cores\n",
    "PRETO = (0, 0, 0)\n",
    "AMARELO = (255, 255, 0)\n",
    "VERDE = (0, 255, 0)\n",
    "AZUL = (0, 255, 255)\n",
    "\n",
    "\n",
    "class Ambiente:\n",
    "    def __init__(self, altura, largura, vel):\n",
    "        self.altura = altura\n",
    "        self.largura = largura\n",
    "        self.vel = vel\n",
    "        self.recompensas = np.zeros((altura // vel, largura // vel))  # Matriz de recompensas\n",
    "        self.obstaculos = []  # Lista de retângulos de obstáculos\n",
    "        \n",
    "\n",
    "        # Definir o objetivo antes de criar o Rect\n",
    "        self.objetivo = (27, 54)  # Posição do objetivo (linha, coluna)\n",
    "        self.recompensas[self.objetivo[0]][self.objetivo[1]] = 10\n",
    "\n",
    "        # Criar um Rect para o objetivo\n",
    "        self.objectivo = pygame.Rect(\n",
    "            self.objetivo[1] * vel,  # Coordenada x\n",
    "            self.objetivo[0] * vel,  # Coordenada y\n",
    "            vel,  # Largura\n",
    "            vel   # Altura\n",
    "        )\n",
    "\n",
    "        # Inicializar o ambiente\n",
    "        self.configurar_ambiente()\n",
    "\n",
    "    def configurar_ambiente(self):\n",
    "        # Definir o objetivo\n",
    "        self.objetivo = (27, 54)  # Posição do objetivo (matriz)\n",
    "        self.recompensas[self.objetivo[0]][self.objetivo[1]] = 10\n",
    "\n",
    "        # Criar os obstáculos (C invertido)\n",
    "        for i in range(21, 45):\n",
    "            self.recompensas[i][32] = -1  # Penalização para bater na parede\n",
    "            self.obstaculos.append(pygame.Rect(32 * VEL, i * VEL, VEL, VEL))\n",
    "\n",
    "        self.obstaculos.append(pygame.Rect(30 * VEL , 21 * VEL, 3 * VEL, VEL))  # Parte superior\n",
    "        self.obstaculos.append(pygame.Rect(30 * VEL , 44 * VEL, 3 * VEL, VEL))  # Parte inferior\n",
    "\n",
    "        # Adicionar margens como obstáculos\n",
    "        margem_espessura = 5  # Ajuste conforme necessário\n",
    "\n",
    "        # Margem superior\n",
    "        self.obstaculos.append(pygame.Rect(0, 0, self.largura + 100, margem_espessura))\n",
    "        # Margem inferior\n",
    "        self.obstaculos.append(pygame.Rect(0, self.altura - 95, self.largura + 100, margem_espessura))\n",
    "        # Margem esquerda\n",
    "        self.obstaculos.append(pygame.Rect(0, 0, margem_espessura, self.altura))\n",
    "        # Margem direita\n",
    "        self.obstaculos.append(pygame.Rect(self.largura + 85, 0, margem_espessura, self.altura))\n",
    "\n",
    "        # Atualizar penalizações para as bordas na matriz de recompensas\n",
    "        for i in range(self.altura // self.vel):  # Altura em passos\n",
    "            for j in range(self.largura // self.vel):  # Largura em passos\n",
    "                # Penalizar células na margem superior ou inferior\n",
    "                if i < margem_espessura // self.vel or i >= (self.altura - margem_espessura) // self.vel:\n",
    "                    self.recompensas[i][j] = -10\n",
    "                # Penalizar células na margem esquerda ou direita\n",
    "                if j < margem_espessura // self.vel or j >= (self.largura - margem_espessura) // self.vel:\n",
    "                    self.recompensas[i][j] = -10\n",
    "\n",
    "    def desenhar(self, tela):\n",
    "        \n",
    "        # Desenhar obstáculos e margem\n",
    "        for obstaculo in self.obstaculos:\n",
    "            pygame.draw.rect(tela, AMARELO, obstaculo)\n",
    "\n",
    "\n",
    "        # Desenhar o objetivo\n",
    "        objetivo_centro = (self.objetivo[1] * VEL + VEL // 2, self.objetivo[0] * VEL + VEL // 2)\n",
    "        pygame.draw.circle(tela, VERDE, objetivo_centro, RAIO_OBJECTIVO)\n",
    "        #self.objectivo = pygame.Rect(self.objetivo[1] * VEL, self.objetivo[0] * VEL, VEL, VEL)\n",
    "\n",
    "    def verificar_colisao(self, x, y):\n",
    "        # Verificar colisão com os obstáculos\n",
    "        for obstaculo in self.obstaculos:\n",
    "            if obstaculo.collidepoint(x, y):\n",
    "                x = x\n",
    "                y = y \n",
    "                return x,y\n",
    "                #return True\n",
    "        return False\n",
    "    \n",
    "    def obter_estado(self, pos):\n",
    "        return (pos[1] // self.vel, pos[0] // self.vel)\n",
    "\n",
    "    def obter_recompensa(self, estado):\n",
    "        if estado[0] < 0 or estado[0] >= self.altura //self.vel or estado[1] < 0 or estado[1] >= self.largura // self.vel:\n",
    "            return - 10\n",
    "        return self.recompensas[estado[0]][estado[1]]\n",
    "    \n",
    "\n",
    "\n",
    "class Agente:\n",
    "    def __init__(self, ambiente):\n",
    "        self.ambiente = ambiente\n",
    "        self.x = 100\n",
    "        self.y = 100\n",
    "        self.mem_aprend = MemoriaEsparsa()\n",
    "        self.accoes = [\n",
    "            Accao(['esquerda']),\n",
    "            Accao(['direita']),\n",
    "            Accao(['cima']),\n",
    "            Accao(['baixo']),\n",
    "        ]\n",
    "\n",
    "        self.sel_accao = EGreedy(self.mem_aprend, self.accoes, EPSILON)\n",
    "        #self.qlearning = QLearning(self.mem_aprend, self.sel_accao, ALFA, GAMA)  #DynaQ(mem_aprend, sel_accao, alfa, gama, num_sim)\n",
    "        self.dynaq = DynaQ(self.mem_aprend, self.sel_accao, ALFA, GAMA, NUM_SIM)\n",
    "        \n",
    "\n",
    "    def mover(self, acao):\n",
    "        move_x, move_y = 0, 0\n",
    "        if acao == 'esquerda':\n",
    "            move_x = -VEL\n",
    "        elif acao == 'direita':\n",
    "            move_x = VEL\n",
    "        elif acao == 'cima':\n",
    "            move_y = -VEL\n",
    "        elif acao == 'baixo':\n",
    "            move_y = VEL\n",
    "\n",
    "        novo_x = self.x + move_x\n",
    "        novo_y = self.y + move_y\n",
    "\n",
    "        margem_espessura = 5\n",
    "        novo_x = max(margem_espessura + RAIO_AGENTE, min(novo_x, self.ambiente.largura - margem_espessura - RAIO_AGENTE))\n",
    "        novo_y = max(margem_espessura + RAIO_AGENTE, min(novo_y, self.ambiente.altura - margem_espessura - RAIO_AGENTE))\n",
    "\n",
    "        \n",
    "        # Verificar colisão com obstáculos\n",
    "        if self.ambiente.verificar_colisao(novo_x, novo_y):\n",
    "            return self.x, self.y  # Não se move ao colidir\n",
    "\n",
    "        return novo_x, novo_y\n",
    "\n",
    "    def treinar(self, num_episodios):\n",
    "        for episodio in range(num_episodios):\n",
    "            self.x, self.y = 100, 100  # Reiniciar posição inicial\n",
    "            estado = self.ambiente.obter_estado((self.x, self.y))\n",
    "            accao = self.sel_accao.seleccionar_accao(estado)\n",
    "            total_recompensa = 0\n",
    "            passos = 0 \n",
    "            for passo in range(500):\n",
    "                novo_x, novo_y = self.mover(accao.accoes[0])\n",
    "                novo_estado = self.ambiente.obter_estado((novo_x, novo_y))\n",
    "                recompensa = self.ambiente.obter_recompensa(novo_estado)\n",
    "                total_recompensa += recompensa # Atualizar recompensa total e passos , para testes no print \n",
    "                \n",
    "                \n",
    "                if self.ambiente.objectivo.colliderect(pygame.Rect(novo_x, novo_y, RAIO_AGENTE, RAIO_AGENTE)):\n",
    "                    recompensa = 10 # Recompensa máxima ao atingir o objetivo\n",
    "                    self.dynaq.aprender(estado,accao, recompensa, novo_estado)\n",
    "                    break  # Encerrar o episódio, mas continuar o treinamento\n",
    "\n",
    "                self.x, self.y = novo_x, novo_y\n",
    "                estado = novo_estado\n",
    "                accao = self.sel_accao.seleccionar_accao(estado)\n",
    "                #self.dynaq.aprender(estado, accao, recompensa, novo_estado)\n",
    "                #print(self.mem_aprend.memoria)\n",
    "                #print(estado)\n",
    "                #print(recompensa)\n",
    " \n",
    "\n",
    "    def executar_politica(self):\n",
    "        estado = self.ambiente.obter_estado((self.x, self.y))\n",
    "        accao = self.sel_accao.seleccionar_accao(estado)  # Política aprendida\n",
    "        self.x, self.y = self.mover(accao.accoes[0])\n",
    "\n",
    "        # Verificar se atingiu o objetivo\n",
    "        if self.ambiente.objectivo.colliderect(pygame.Rect(self.x, self.y, RAIO_AGENTE, RAIO_AGENTE)):\n",
    "            #print(\"Objetivo alcançado! Reiniciando para o início.\")\n",
    "            self.x, self.y = 100, 100  # Reiniciar para o início\n",
    "\n",
    "\n",
    "\n",
    "# Configuração do jogo\n",
    "pygame.init()\n",
    "screen = pygame.display.set_mode((ALTURA, LARGURA))\n",
    "pygame.display.set_caption(\"Aprendizagem por Reforço\")\n",
    "relogio = pygame.time.Clock()\n",
    "\n",
    "# Inicializar ambiente e agente\n",
    "ambiente = Ambiente(ALTURA, LARGURA, VEL)\n",
    "agente = Agente(ambiente)\n",
    "\n",
    "# Treinamento do agente\n",
    "agente.treinar(NUM_EPISODIOS)\n",
    "\n",
    "# Loop principal\n",
    "while True:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == QUIT:\n",
    "            pygame.quit()\n",
    "            exit()\n",
    "\n",
    "    # Agente segue a política aprendida\n",
    "    agente.executar_politica()\n",
    "\n",
    "    # Desenhar na tela\n",
    "    screen.fill(PRETO)\n",
    "    ambiente.desenhar(screen)\n",
    "    pygame.draw.circle(screen, AZUL, (agente.x, agente.y), RAIO_AGENTE)\n",
    "    pygame.display.update()\n",
    "    relogio.tick(30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objetivo alcançado! Reiniciando...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# versão com erro obstaculos \n",
    "\n",
    "\n",
    "\n",
    "import pygame\n",
    "from pygame.locals import *\n",
    "from sys import exit\n",
    "import random\n",
    "import numpy as np\n",
    "from pratica_pt2 import Accao, MemoriaEsparsa, EGreedy, QLearning, DynaQ\n",
    "\n",
    "# Parâmetros de aprendizagem\n",
    "NUM_EPISODIOS = 1000\n",
    "NUM_SIM = 10\n",
    "ALFA = 0.1\n",
    "GAMA = 0.9\n",
    "EPSILON = 0.1\n",
    "MAX_PASSOS = 200  # Limite máximo de passos por episódio\n",
    "VELOCIDADE = 2  # Parâmetro de velocidade da bolinha\n",
    "RAIO_AGENTE = 20  # Raio do agente (bolinha)\n",
    "\n",
    "class Ambiente:\n",
    "    def __init__(self):\n",
    "        self.height = 640  \n",
    "        self.width = 550\n",
    "        self.screen = pygame.display.set_mode((self.height, self.width))\n",
    "        pygame.display.set_caption(\"Aprendizagem por reforço\")\n",
    "        self.vel = 10\n",
    "        self.agente_width = RAIO_AGENTE\n",
    "        self.agente_height = RAIO_AGENTE\n",
    "        self.recompensas = np.zeros((self.height // self.vel, self.width // self.vel))\n",
    "        self.recompensas[26][54] = 10  # Objetivo\n",
    "        for i in range(21, 35):\n",
    "            self.recompensas[i][32] = -1  # Obstáculos C invertido\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.x, self.y = 100, 100\n",
    "        return self.obter_estado((self.x, self.y))\n",
    "\n",
    "    def obter_estado(self, pos):\n",
    "        return (min(pos[0] // self.vel, self.width // self.vel - 1), min(pos[1] // self.vel, self.height // self.vel - 1))\n",
    "\n",
    "    def passo(self, acao):\n",
    "        move_x, move_y = 0, 0\n",
    "        if acao == 'esquerda':\n",
    "            move_x = -self.vel\n",
    "        elif acao == 'direita':\n",
    "            move_x = self.vel\n",
    "        elif acao == 'cima':\n",
    "            move_y = -self.vel\n",
    "        elif acao == 'baixo':\n",
    "            move_y = self.vel\n",
    "\n",
    "        new_x = min(max(self.x + move_x, 0), self.width - self.agente_width)\n",
    "        new_y = min(max(self.y + move_y, 0), self.height - self.agente_height)\n",
    "\n",
    "        # Verifica colisão com obstáculos\n",
    "        if self.colisao((new_x, new_y)):\n",
    "            new_x, new_y = self.x, self.y\n",
    "\n",
    "        self.x, self.y = new_x, new_y\n",
    "        novo_estado = self.obter_estado((self.x, self.y))\n",
    "        recompensa = self.recompensas[novo_estado[1]][novo_estado[0]]  # Corrigido para acessar [linha][coluna]\n",
    "        return novo_estado, recompensa\n",
    "\n",
    "    def colisao(self, pos):\n",
    "        return (self.width // 2 + 40 <= pos[0] <= self.width // 2 + 90 and self.height // 2 - 200 <= pos[1] <= self.height // 2 + 100) or \\\n",
    "               (self.width // 2 - 40 <= pos[0] <= self.width // 2 + 40 and self.height // 2 - 200 <= pos[1] <= self.height // self.height // 2 - 170) or \\\n",
    "               (self.width // 2 - 40 <= pos[0] <= self.width // 2 + 40 and self.height // 2 + 70 <= pos[1] <= self.height // 2 + 100)\n",
    "\n",
    "    def desenhar(self):\n",
    "        self.screen.fill((0, 0, 0))\n",
    "        pygame.draw.rect(self.screen, (255,255,0), (self.width // 2 + 40 , self.height // 2 - 200, 50, 300))\n",
    "        pygame.draw.rect(self.screen, (255,255,0), (self.width // 2 - 40, self.height // 2 - 200, 80, 30))\n",
    "        pygame.draw.rect(self.screen, (255,255,0), (self.width // 2 - 40, self.height // 2 + 70, 80, 30))\n",
    "        pygame.draw.circle(self.screen, (0,255,0), (550, 260), 30)\n",
    "        pygame.draw.circle(self.screen, (0,255,255), (self.x, self.y), self.agente_width)\n",
    "        pygame.display.update()\n",
    "\n",
    "    def objectivo_colisao(self, pos):\n",
    "        return pygame.Rect(pos[0], pos[1], RAIO_AGENTE, RAIO_AGENTE).colliderect(pygame.Rect(520, 230, 60, 60))\n",
    "\n",
    "class Agente:\n",
    "    def __init__(self, ambiente):\n",
    "        self.ambiente = ambiente\n",
    "        self.mem_aprend = MemoriaEsparsa()\n",
    "        self.accoes = [Accao(['esquerda']), Accao(['direita']), Accao(['cima']), Accao(['baixo'])]\n",
    "        self.sel_accao = EGreedy(self.mem_aprend, self.accoes, EPSILON)\n",
    "        self.agente = QLearning(self.mem_aprend, self.sel_accao, ALFA, GAMA)\n",
    "        self.num_episodios = NUM_EPISODIOS\n",
    "        self.velocidade = VELOCIDADE\n",
    "\n",
    "    def treinar(self):\n",
    "        for episodio in range(self.num_episodios):\n",
    "            estado = self.ambiente.reset()\n",
    "            accao = self.sel_accao.seleccionar_accao(estado)\n",
    "            passos = 0\n",
    "            while True:\n",
    "                novo_estado, recompensa = self.ambiente.passo(accao.accoes[0])\n",
    "                nova_accao = self.sel_accao.seleccionar_accao(novo_estado)\n",
    "                self.agente.aprender(estado, accao, recompensa, novo_estado)\n",
    "                estado = novo_estado\n",
    "                accao = nova_accao\n",
    "                passos += 1\n",
    "                if recompensa == 10 or recompensa == -10 or passos >= MAX_PASSOS:  # Objetivo alcançado, penalidade, ou limite de passos\n",
    "                    break\n",
    "            #print(f\"Episódio {episodio+1}/{self.num_episodios}: Passos = {passos}, Recompensa = {recompensa}\")\n",
    "\n",
    "    def executar(self):\n",
    "        estado = self.ambiente.reset()\n",
    "        while True:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == QUIT:\n",
    "                    pygame.quit()\n",
    "                    exit()\n",
    "            accao = self.sel_accao.seleccionar_accao(estado)\n",
    "            novo_estado, recompensa = self.ambiente.passo(accao.accoes[0])\n",
    "            self.agente.aprender(estado, accao, recompensa, novo_estado)  # Chama o método aprender ao atingir o objetivo\n",
    "            estado = novo_estado\n",
    "            self.ambiente.desenhar()\n",
    "            if self.ambiente.objectivo_colisao((self.ambiente.x, self.ambiente.y)):\n",
    "                print(\"Objetivo alcançado! Reiniciando...\")\n",
    "                estado = self.ambiente.reset()\n",
    "            pygame.time.delay(int(100 / self.velocidade))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pygame.init()\n",
    "    ambiente = Ambiente()\n",
    "    agente = Agente(ambiente)\n",
    "    agente.treinar()\n",
    "    agente.executar()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
